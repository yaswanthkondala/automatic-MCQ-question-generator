{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50af007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import logging\n",
    "from typing import Optional, Dict, Union\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "import torch\n",
    "from transformers import(\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class QGPipeline:\n",
    "    \"\"\"Poor man's QG pipeline\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        ans_model: PreTrainedModel,\n",
    "        ans_tokenizer: PreTrainedTokenizer,\n",
    "        qg_format: str,\n",
    "        use_cuda: bool\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.ans_model = ans_model\n",
    "        self.ans_tokenizer = ans_tokenizer\n",
    "\n",
    "        self.qg_format = qg_format\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        if self.ans_model is not self.model:\n",
    "            self.ans_model.to(self.device)\n",
    "\n",
    "        assert self.model.__class__.__name__ in [\"T5ForConditionalGeneration\", \"BartForConditionalGeneration\"]\n",
    "\n",
    "        if \"T5ForConditionalGeneration\" in self.model.__class__.__name__:\n",
    "            self.model_type = \"t5\"\n",
    "        else:\n",
    "            self.model_type = \"bart\"\n",
    "\n",
    "    def __call__(self, inputs: str):\n",
    "        inputs = \" \".join(inputs.split())\n",
    "        sents, answers = self._extract_answers(inputs)\n",
    "        flat_answers = list(itertools.chain(*answers))\n",
    "\n",
    "        if len(flat_answers) == 0:\n",
    "            return []\n",
    "\n",
    "        if self.qg_format == \"prepend\":\n",
    "            qg_examples = self._prepare_inputs_for_qg_from_answers_prepend(inputs, answers)\n",
    "        else:\n",
    "            qg_examples = self._prepare_inputs_for_qg_from_answers_hl(sents, answers)\n",
    "\n",
    "        qg_inputs = [example['source_text'] for example in qg_examples]\n",
    "        questions = self._generate_questions(qg_inputs)\n",
    "        output = [{'answer': example['answer'], 'question': que} for example, que in zip(qg_examples, questions)]\n",
    "        return output\n",
    "\n",
    "    def _generate_questions(self, inputs):\n",
    "        inputs = self._tokenize(inputs, padding=True, truncation=True)\n",
    "\n",
    "        outs = self.model.generate(\n",
    "            input_ids=inputs['input_ids'].to(self.device),\n",
    "            attention_mask=inputs['attention_mask'].to(self.device),\n",
    "            max_length=32,\n",
    "            num_beams=4,\n",
    "        )\n",
    "\n",
    "        questions = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
    "        return questions\n",
    "\n",
    "    def _extract_answers(self, context):\n",
    "        sents, inputs = self._prepare_inputs_for_ans_extraction(context)\n",
    "        inputs = self._tokenize(inputs, padding=True, truncation=True)\n",
    "\n",
    "        outs = self.ans_model.generate(\n",
    "            input_ids=inputs['input_ids'].to(self.device),\n",
    "            attention_mask=inputs['attention_mask'].to(self.device),\n",
    "            max_length=32,\n",
    "        )\n",
    "\n",
    "        dec = [self.ans_tokenizer.decode(ids, skip_special_tokens=False) for ids in outs]\n",
    "        answers = [item.split('<sep>') for item in dec]\n",
    "        answers = [i[:-1] for i in answers]\n",
    "\n",
    "        return sents, answers\n",
    "\n",
    "    def _tokenize(self,\n",
    "                  inputs,\n",
    "                  padding=True,\n",
    "                  truncation=True,\n",
    "                  add_special_tokens=True,\n",
    "                  max_length=512\n",
    "                  ):\n",
    "        inputs = self.tokenizer.batch_encode_plus(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            truncation=truncation,\n",
    "            padding=\"max_length\" if padding else False,\n",
    "            pad_to_max_length=padding,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return inputs\n",
    "\n",
    "    def _prepare_inputs_for_ans_extraction(self, text):\n",
    "        sents = sent_tokenize(text)\n",
    "\n",
    "        inputs = []\n",
    "        for i in range(len(sents)):\n",
    "            source_text = \"extract answers:\"\n",
    "            for j, sent in enumerate(sents):\n",
    "                if i == j:\n",
    "                    sent = \"<hl> %s <hl>\" % sent\n",
    "                source_text = \"%s %s\" % (source_text, sent)\n",
    "                source_text = source_text.strip()\n",
    "\n",
    "            if self.model_type == \"t5\":\n",
    "                source_text = source_text + \" </s>\"\n",
    "            inputs.append(source_text)\n",
    "\n",
    "        return sents, inputs\n",
    "\n",
    "    def _prepare_inputs_for_qg_from_answers_hl(self, sents, answers):\n",
    "        inputs = []\n",
    "        for i, answer in enumerate(answers):\n",
    "            if len(answer) == 0:\n",
    "                continue\n",
    "            sent = sents[i].lower()\n",
    "            for answer_text in answer:\n",
    "                sents_copy = sents[:]\n",
    "\n",
    "                answer_text = answer_text.strip().lower()\n",
    "                answer_text = re.sub(\"<pad> | <pad>\", \"\", answer_text)\n",
    "\n",
    "                ans_start_idx = sent.index(answer_text)\n",
    "\n",
    "                sent = f\"{sent[:ans_start_idx]} <hl> {answer_text} <hl> {sent[ans_start_idx + len(answer_text): ]}\"\n",
    "                sents_copy[i] = sent\n",
    "\n",
    "                source_text = \" \".join(sents_copy)\n",
    "                source_text = f\"generate question: {source_text}\"\n",
    "                if self.model_type == \"t5\":\n",
    "                    source_text = source_text + \" </s>\"\n",
    "\n",
    "                inputs.append({\"answer\": answer_text, \"source_text\": source_text})\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def _prepare_inputs_for_qg_from_answers_prepend(self, context, answers):\n",
    "        flat_answers = list(itertools.chain(*answers))\n",
    "        examples = []\n",
    "        for answer in flat_answers:\n",
    "            source_text = f\"answer: {answer} context: {context}\"\n",
    "            if self.model_type == \"t5\":\n",
    "                source_text = source_text + \" </s>\"\n",
    "\n",
    "            examples.append({\"answer\": answer, \"source_text\": source_text})\n",
    "        return examples\n",
    "\n",
    "\n",
    "class MultiTaskQAQGPipeline(QGPipeline):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def __call__(self, inputs: Union[Dict, str]):\n",
    "        if type(inputs) is str:\n",
    "            # do qg\n",
    "            return super().__call__(inputs)\n",
    "        else:\n",
    "            # do qa\n",
    "            return self._extract_answer(inputs[\"question\"], inputs[\"context\"])\n",
    "\n",
    "    def _prepare_inputs_for_qa(self, question, context):\n",
    "        source_text = f\"question: {question}  context: {context}\"\n",
    "        if self.model_type == \"t5\":\n",
    "            source_text = source_text + \" </s>\"\n",
    "        return source_text\n",
    "\n",
    "    def _extract_answer(self, question, context):\n",
    "        source_text = self._prepare_inputs_for_qa(question, context)\n",
    "        inputs = self._tokenize([source_text], padding=False)\n",
    "\n",
    "        outs = self.model.generate(\n",
    "            input_ids=inputs['input_ids'].to(self.device),\n",
    "            attention_mask=inputs['attention_mask'].to(self.device),\n",
    "            max_length=16,\n",
    "        )\n",
    "\n",
    "        answer = self.tokenizer.decode(outs[0], skip_special_tokens=True)\n",
    "        return answer\n",
    "\n",
    "\n",
    "class E2EQGPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        use_cuda: bool\n",
    "    ):\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        assert self.model.__class__.__name__ in [\"T5ForConditionalGeneration\", \"BartForConditionalGeneration\"]\n",
    "\n",
    "        if \"T5ForConditionalGeneration\" in self.model.__class__.__name__:\n",
    "            self.model_type = \"t5\"\n",
    "        else:\n",
    "            self.model_type = \"bart\"\n",
    "\n",
    "        self.default_generate_kwargs = {\n",
    "            \"max_length\": 256,\n",
    "            \"num_beams\": 4,\n",
    "            \"length_penalty\": 1.5,\n",
    "            \"no_repeat_ngram_size\": 3,\n",
    "            \"early_stopping\": True,\n",
    "        }\n",
    "\n",
    "    def __call__(self, context: str, **generate_kwargs):\n",
    "        inputs = self._prepare_inputs_for_e2e_qg(context)\n",
    "\n",
    "        # TODO: when overrding default_generate_kwargs all other arguments need to be passsed\n",
    "        # find a better way to do this\n",
    "        if not generate_kwargs:\n",
    "            generate_kwargs = self.default_generate_kwargs\n",
    "\n",
    "        input_length = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "        # max_length = generate_kwargs.get(\"max_length\", 256)\n",
    "        # if input_length < max_length:\n",
    "        #     logger.warning(\n",
    "        #         \"Your max_length is set to {}, but you input_length is only {}. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\".format(\n",
    "        #             max_length, input_length\n",
    "        #         )\n",
    "        #     )\n",
    "\n",
    "        outs = self.model.generate(\n",
    "            input_ids=inputs['input_ids'].to(self.device),\n",
    "            attention_mask=inputs['attention_mask'].to(self.device),\n",
    "            **generate_kwargs\n",
    "        )\n",
    "\n",
    "        prediction = self.tokenizer.decode(outs[0], skip_special_tokens=True)\n",
    "        questions = prediction.split(\"<sep>\")\n",
    "        questions = [question.strip() for question in questions[:-1]]\n",
    "        return questions\n",
    "\n",
    "    def _prepare_inputs_for_e2e_qg(self, context):\n",
    "        source_text = f\"generate questions: {context}\"\n",
    "        if self.model_type == \"t5\":\n",
    "            source_text = source_text + \" </s>\"\n",
    "\n",
    "        inputs = self._tokenize([source_text], padding=False)\n",
    "        return inputs\n",
    "\n",
    "    def _tokenize(\n",
    "        self,\n",
    "        inputs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512\n",
    "    ):\n",
    "        inputs = self.tokenizer.batch_encode_plus(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            truncation=truncation,\n",
    "            padding=\"max_length\" if padding else False,\n",
    "            pad_to_max_length=padding,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return inputs\n",
    "\n",
    "\n",
    "SUPPORTED_TASKS = {\n",
    "    \"text2text-generation\": {\n",
    "        \"impl\": QGPipeline,\n",
    "        \"default\": {\n",
    "            \"model\": \"valhalla/t5-small-qg-hl\",\n",
    "            \"ans_model\": \"valhalla/t5-small-qa-qg-hl\",\n",
    "        }\n",
    "    },\n",
    "    \"multitask-qa-qg\": {\n",
    "        \"impl\": MultiTaskQAQGPipeline,\n",
    "        \"default\": {\n",
    "            \"model\": \"valhalla/t5-small-qa-qg-hl\",\n",
    "        }\n",
    "    },\n",
    "    \"e2e-qg\": {\n",
    "        \"impl\": E2EQGPipeline,\n",
    "        \"default\": {\n",
    "            \"model\": \"valhalla/t5-small-e2e-qg\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def pipeline(\n",
    "    task: str,\n",
    "    model: Optional = None,\n",
    "    tokenizer: Optional[Union[str, PreTrainedTokenizer]] = None,\n",
    "    qg_format: Optional[str] = \"highlight\",\n",
    "    ans_model: Optional= None,\n",
    "    ans_tokenizer: Optional[Union[str, PreTrainedTokenizer]] = None,\n",
    "    use_cuda: Optional[bool] = True,\n",
    "    **kwargs,\n",
    "):\n",
    "    # Retrieve the task\n",
    "    if task not in SUPPORTED_TASKS:\n",
    "        raise KeyError(\"Unknown task {}, available tasks are {}\".format(task, list(SUPPORTED_TASKS.keys())))\n",
    "\n",
    "    targeted_task = SUPPORTED_TASKS[task]\n",
    "    task_class = targeted_task[\"impl\"]\n",
    "\n",
    "    # Use default model/config/tokenizer for the task if no model is provided\n",
    "    if model is None:\n",
    "        model = targeted_task[\"default\"][\"model\"]\n",
    "\n",
    "    # Try to infer tokenizer from model or config name (if provided as str)\n",
    "    if tokenizer is None:\n",
    "        if isinstance(model, str):\n",
    "            tokenizer = model\n",
    "        else:\n",
    "            # Impossible to guest what is the right tokenizer here\n",
    "            raise Exception(\n",
    "                \"Impossible to guess which tokenizer to use. \"\n",
    "                \"Please provided a PretrainedTokenizer class or a path/identifier to a pretrained tokenizer.\"\n",
    "            )\n",
    "\n",
    "    # Instantiate tokenizer if needed\n",
    "    if isinstance(tokenizer, (str, tuple)):\n",
    "        if isinstance(tokenizer, tuple):\n",
    "            # For tuple we have (tokenizer name, {kwargs})\n",
    "            tokenizer = AutoTokenizer.from_pretrained(tokenizer[0], **tokenizer[1])\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "\n",
    "    # Instantiate model if needed\n",
    "    if isinstance(model, str):\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model)\n",
    "\n",
    "    if task == \"text2text-generation\":\n",
    "        if ans_model is None:\n",
    "            # load default ans model\n",
    "            ans_model = targeted_task[\"default\"][\"ans_model\"]\n",
    "            ans_tokenizer = AutoTokenizer.from_pretrained(ans_model)\n",
    "            ans_model = AutoModelForSeq2SeqLM.from_pretrained(ans_model)\n",
    "        else:\n",
    "            # Try to infer tokenizer from model or config name (if provided as str)\n",
    "            if ans_tokenizer is None:\n",
    "                if isinstance(ans_model, str):\n",
    "                    ans_tokenizer = ans_model\n",
    "                else:\n",
    "                    # Impossible to guest what is the right tokenizer here\n",
    "                    raise Exception(\n",
    "                        \"Impossible to guess which tokenizer to use. \"\n",
    "                        \"Please provided a PretrainedTokenizer class or a path/identifier to a pretrained tokenizer.\"\n",
    "                    )\n",
    "\n",
    "            # Instantiate tokenizer if needed\n",
    "            if isinstance(ans_tokenizer, (str, tuple)):\n",
    "                if isinstance(ans_tokenizer, tuple):\n",
    "                    # For tuple we have (tokenizer name, {kwargs})\n",
    "                    ans_tokenizer = AutoTokenizer.from_pretrained(ans_tokenizer[0], **ans_tokenizer[1])\n",
    "                else:\n",
    "                    ans_tokenizer = AutoTokenizer.from_pretrained(ans_tokenizer)\n",
    "\n",
    "            if isinstance(ans_model, str):\n",
    "                ans_model = AutoModelForSeq2SeqLM.from_pretrained(ans_model)\n",
    "\n",
    "    if task == \"e2e-qg\":\n",
    "        return task_class(model=model, tokenizer=tokenizer, use_cuda=use_cuda)\n",
    "    elif task == \"text2text-generation\":\n",
    "        return task_class(model=model, tokenizer=tokenizer, ans_model=ans_model, ans_tokenizer=ans_tokenizer, qg_format=qg_format, use_cuda=use_cuda)\n",
    "    else:\n",
    "        return task_class(model=model, tokenizer=tokenizer, ans_model=model, ans_tokenizer=tokenizer, qg_format=qg_format, use_cuda=use_cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb4b1488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\anakonda\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Unknown task text2text-generation, available tasks are ['question-generation', 'multitask-qa-qg', 'e2e-qg']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s2v\n\u001b[0;32m     13\u001b[0m spacy_nlp\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_components\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m t5_generator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext2text-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#t2t_generator = TextGenerator(output_type=\"question\")\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_from_T5\u001b[39m(context, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n",
      "File \u001b[1;32m~\\Downloads\\Multiple-Choice-Question-Generation-T5-and-Text2Text-master\\pipelines.py:321\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, tokenizer, qg_format, ans_model, ans_tokenizer, use_cuda, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpipeline\u001b[39m(\n\u001b[0;32m    310\u001b[0m     task: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    311\u001b[0m     model: Optional \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    318\u001b[0m ):\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;66;03m# Retrieve the task\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SUPPORTED_TASKS:\n\u001b[1;32m--> 321\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown task \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, available tasks are \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(task, \u001b[38;5;28mlist\u001b[39m(SUPPORTED_TASKS\u001b[38;5;241m.\u001b[39mkeys())))\n\u001b[0;32m    323\u001b[0m     targeted_task \u001b[38;5;241m=\u001b[39m SUPPORTED_TASKS[task]\n\u001b[0;32m    324\u001b[0m     task_class \u001b[38;5;241m=\u001b[39m targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Unknown task text2text-generation, available tasks are ['question-generation', 'multitask-qa-qg', 'e2e-qg']\""
     ]
    }
   ],
   "source": [
    "from pipelines import pipeline\n",
    "from transformers.pipelines.text2text_generation import Text2TextGenerationPipeline\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import spacy\n",
    "from sense2vec import Sense2VecComponent\n",
    "\n",
    "spacy_nlp = spacy.blank(\"en\")\n",
    "s2v = Sense2VecComponent(spacy_nlp.vocab).from_disk(\"./s2v_old\")\n",
    "@spacy_nlp.component(\"my_components\")\n",
    "def my_components(s2v):\n",
    "    return s2v\n",
    "spacy_nlp.add_pipe(\"my_components\")\n",
    "\n",
    "t5_generator = pipeline(\"text2text-generation\")\n",
    "#t2t_generator = TextGenerator(output_type=\"question\")\n",
    "\n",
    "\n",
    "def generate_from_T5(context, n=5):\n",
    "    res = t5_generator(context)\n",
    "    ans = []\n",
    "    que = []\n",
    "    for i, r in enumerate(res):\n",
    "        if i < n:\n",
    "            ans.append(r['answer'])\n",
    "            que.append(r['question'])\n",
    "    return que, ans\n",
    "\n",
    "\n",
    "# def generate_from_t2t(context, n=5):\n",
    "#     res = t2t_generator.predict([context] * n)\n",
    "#     ans = []\n",
    "#     que = []\n",
    "#     for r in res:\n",
    "#         if r[0] not in que:\n",
    "#             que.append(r[0])\n",
    "#             ans.append(r[1])\n",
    "#     return que, ans\n",
    "\n",
    "\n",
    "# q1, a1 = generate_from_T5(text5)\n",
    "# q2, a2 = generate_from_t2t(text5)\n",
    "\n",
    "\n",
    "# %%\n",
    "def generate_distractor(sentence, n=5):\n",
    "    doc = spacy_nlp(sentence)\n",
    "    ans = [doc]\n",
    "    for ent in doc:\n",
    "        try:\n",
    "            assert ent._.in_s2v\n",
    "            most_similar = ent._.s2v_most_similar(10)\n",
    "            for m in most_similar:\n",
    "                text = m[0][0].lower()\n",
    "                if text not in ans:\n",
    "                    ans.append((text, False))\n",
    "            return ans[-n:]\n",
    "        except:\n",
    "            return ([('none', False)] * n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "751e9fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text=\" Conventionally, the term is reserved for two major international conflicts that occurred during the first half of the 20th century, .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df77c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5151ea92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "max_answers=5\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43011456",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "questions_t5, _answers_t5 = models.generate_from_T5(input_text, n=max_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08602ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What is an international conflict that involves most or all of the world's major powers?\",\n",
       " 'When was world war i?',\n",
       " 'When was world war ii?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "questions_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4746773c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a world war', '1914–1918', '1939–1945']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_answers_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a9cb408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_T5(context, n=5):\n",
    "    res = models.t5_generator(context)\n",
    "    ans = []\n",
    "    que = []\n",
    "    for i, r in enumerate(res):\n",
    "        if i < n:\n",
    "            ans.append(r['answer'])\n",
    "            que.append(r['question'])\n",
    "    return que, ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f9647cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q,a=generate_from_T5(input_text,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aaa5ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28731e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_t5, _answers_t5 = models.generate_from_T5(input_text, n=max_answers)\n",
    "answers_t5 = []\n",
    "for a in _answers_t5:\n",
    "        dis = models.generate_distractor(a, 4)\n",
    "        ans = [(a, True)] + dis\n",
    "        random.shuffle(ans)\n",
    "        answers_t5.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "912a6661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('none', False),\n",
       "  ('none', False),\n",
       "  ('the first half of the 20th century', True),\n",
       "  ('none', False),\n",
       "  ('none', False)]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddac7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sense2vec import Sense2Vec\n",
    "s2v = Sense2Vec().from_disk('s2v_old')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4872e4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word  the_first_half_of_the_20th_century\n",
      "Best sense  None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m sense \u001b[38;5;241m=\u001b[39m s2v\u001b[38;5;241m.\u001b[39mget_best_sense(word)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest sense \u001b[39m\u001b[38;5;124m\"\u001b[39m,sense)\n\u001b[1;32m----> 9\u001b[0m most_similar \u001b[38;5;241m=\u001b[39m \u001b[43ms2v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43msense\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m (most_similar)\n\u001b[0;32m     12\u001b[0m distractors \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mD:\\anakonda\\lib\\site-packages\\sense2vec\\sense2vec.py:208\u001b[0m, in \u001b[0;36mSense2Vec.most_similar\u001b[1;34m(self, keys, n, batch_size)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(keys, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m)):\n\u001b[0;32m    207\u001b[0m     keys \u001b[38;5;241m=\u001b[39m [keys]\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find key \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "originalword = \"the first half of the 20th century\"\n",
    "word = originalword.lower()\n",
    "word = word.replace(\" \", \"_\")\n",
    "\n",
    "print (\"word \",word)\n",
    "sense = s2v.get_best_sense(word)\n",
    "\n",
    "print (\"Best sense \",sense)\n",
    "most_similar = s2v.most_similar(sense, n=20)\n",
    "print (most_similar)\n",
    "\n",
    "distractors = []\n",
    "\n",
    "for each_word in most_similar:\n",
    "  append_word = each_word[0].split(\"|\")[0].replace(\"_\", \" \")\n",
    "  if append_word not in distractors and append_word != originalword:\n",
    "      distractors.append(append_word)\n",
    "\n",
    "print (distractors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07bb811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "originalword=\"barak obama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7419aa7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'distractors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m   distractor_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(candidate_distractors)\n\u001b[0;32m      7\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m answer_embedding,distractor_embeddings\n\u001b[1;32m---> 10\u001b[0m \u001b[43mdistractors\u001b[49m\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m,originalword)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m (distractors)\n\u001b[0;32m     13\u001b[0m answer_embedd, distractor_embedds \u001b[38;5;241m=\u001b[39m get_answer_and_distractor_embeddings(originalword,distractors)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'distractors' is not defined"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model= SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "def get_answer_and_distractor_embeddings(answer,candidate_distractors):\n",
    "  answer_embedding = model.encode([answer])\n",
    "  distractor_embeddings = model.encode(candidate_distractors)\n",
    "  return answer_embedding,distractor_embeddings\n",
    "\n",
    "\n",
    "distractors.insert(0,originalword)\n",
    "print (distractors)\n",
    "\n",
    "answer_embedd, distractor_embedds = get_answer_and_distractor_embeddings(originalword,distractors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5149762e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yaswanth\n",
      "------------------->\n",
      "Dick Cheney\n",
      "the first half of the 20th century\n",
      "Hillary Clinton\n",
      "former president\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def mmr(doc_embedding: np.ndarray,\n",
    "        word_embeddings: np.ndarray,\n",
    "        words: List[str],\n",
    "        top_n: int = 5,\n",
    "        diversity: float = 0.9) -> List[Tuple[str, float]]:\n",
    "    \"\"\" Calculate Maximal Marginal Relevance (MMR)\n",
    "    between candidate keywords and the document.\n",
    "\n",
    "\n",
    "    MMR considers the similarity of keywords/keyphrases with the\n",
    "    document, along with the similarity of already selected\n",
    "    keywords and keyphrases. This results in a selection of keywords\n",
    "    that maximize their within diversity with respect to the document.\n",
    "\n",
    "    Arguments:\n",
    "        doc_embedding: The document embeddings\n",
    "        word_embeddings: The embeddings of the selected candidate keywords/phrases\n",
    "        words: The selected candidate keywords/keyphrases\n",
    "        top_n: The number of keywords/keyhprases to return\n",
    "        diversity: How diverse the select keywords/keyphrases are.\n",
    "                   Values between 0 and 1 with 0 being not diverse at all\n",
    "                   and 1 being most diverse.\n",
    "\n",
    "    Returns:\n",
    "         List[Tuple[str, float]]: The selected keywords/keyphrases with their distances\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [(words[idx], round(float(word_doc_similarity.reshape(1, -1)[0][idx]), 4)) for idx in keywords_idx]\n",
    "  \n",
    "final_distractors = mmr(answer_embedd,distractor_embedds,distractors,5)\n",
    "filtered_distractors = []\n",
    "for dist in final_distractors:\n",
    "  filtered_distractors.append (dist[0])\n",
    "  \n",
    "Answer = filtered_distractors[0]\n",
    "Filtered_Distractors =  filtered_distractors[1:]\n",
    "\n",
    "print (Answer)\n",
    "print (\"------------------->\")\n",
    "for k in Filtered_Distractors:\n",
    "  print (k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe122a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2v = Sense2VecComponent(nlp.vocab).from_disk(\"./s2v_old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f3b421a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.my_components(s2v)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@nlp.component(\"my_components\")\n",
    "def my_components(s2v):\n",
    "    return doc\n",
    "nlp.add_pipe(\"my_components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b0aff3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping pipelines as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b76361ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097567e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
